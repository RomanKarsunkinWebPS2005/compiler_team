## Модуль лексера: устройство, типы и решения

Этот документ описывает реализацию модуля лексического анализа (`src/Lexer`): все типы и функции, их назначения, используемые в коде конструкции C# и принятые решения. Текст сверен со спецификацией `docs/specification/lexical-structure.md` и тестами `tests/Lexer.UnitTests`.

### Обзор

- Вход: исходный текст программы (строка).
- Выход: последовательность токенов (лексем) с типами и позициями.
- Лексер идёт слева направо без отката назад, допускает просмотр вперёд, нормализует переводы строк к `\n`.
- Категории токенов: ключевые слова, идентификаторы, числовые и строковые литералы, операторы, разделители, комментарии, пробелы.

### Ключевые типы

- `TokenType` — перечисление типов токенов: `Identifier`, `Keyword`, `NumberLiteral`, `StringLiteral`, `Operator`, `Delimiter`, `Comment`, `Whitespace`, `EndOfFile`, `Unknown`.

- `public readonly record struct Token(TokenType Type, string Lexeme, int Position)`
  - `public` — тип доступен в других сборках.
  - `record struct` — это value-type (структура) с семантикой значений и авто‑генерацией: сравнение по значению, деконструктор, удобный `ToString`. Отличия:
    - `class` — ссылочный тип (хранится в куче, сравнение по ссылке по умолчанию);
    - `struct` — значимый тип (на стеке/в полях, сравнение по полям при `record struct`);
    - `record` — добавляет семантику «данных» (value‑based equality) и лаконичный синтаксис.
  - `readonly` — поля (позиционные свойства) неизменны после создания; это защищает от случайных модификаций и позволяет компилятору эффективнее работать со значениями.
  - Позиционные параметры:
    - `Type` — категория токена (`TokenType`).
    - `Lexeme` — исходный текст лексемы (как встречен во входной строке); для строк `!…!` — вместе с ограничителями, для кавычек `"…"` — вместе с кавычками.
    - `Position` — индекс начала лексемы в исходном тексте (0‑based, после нормализации перевода строк к `\n`).
  - `override ToString()` — возвращает удобное представление токена, чтобы в логах/ассертах было видно тип, текст и позицию.

- `Lexer` — класс, выполняющий токенизацию. Хранит:
  - исходный текст (`_source`),
  - текущую позицию (`_position`),
  - накопитель токенов.

- `static class LexicalStats` — утилита для подсчёта статистики по токенам из файла. Внутри определён вспомогательный `struct LexicalData` (измеримые поля и форматированный `ToString`).

- `class LexerException : Exception` — исключение лексера с полем `Position`. Позволяет сигнализировать об ошибках лексинга (незакрытая строка/комментарий и т.п.). В текущей версии используется ограниченно (лексер толерантен к некоторым незавершённым конструкциям). Ключевые конструкторы исключений по хорошей практике: параметрless, `(string)`, `(string, Exception)`, десериализация — в случае необходимости можно добавить.

- `class TokenValue` — контейнер для «значения» токена (текст/число/булево). Сейчас не задействован лексером напрямую (лексер работает на уровне лексем, без семантического преобразования), но полезен для последующих этапов (парсер/интерпретатор). Свойства:
  - `string? Text`, `double? Number`, `bool? Boolean`; конструкторы по каждому варианту; `ToString()` — приоритетно печатает имеющееся значение.

- `LexicalStats` — вспомогательный класс для подсчёта статистики лексем по файлу (по ТЗ).

- `LexerException` — исключение с позицией (зарезервировано для случаев, где потребуется строгая ошибка лексинга; в текущей версии практически не используется, так как лексер толерантен к некоторым незавершённым конструкциям).

### Маршрут (проход) лексера

1. Нормализация перевода строк: `\r\n` и `\r` приводятся к `\n`, чтобы позиции совпадали везде.
2. Основной цикл, пока не конец входа:
   - Пробел/таб/перевод строки → собираем группу whitespace и отдаём токен `Whitespace`.
   - `//` → до конца строки → `Comment`.
   - `/*` → до `*/` (или до конца входа, если нет закрытия) → `Comment`.
   - Буква/`_` → «словоподобный» токен: считываем буквы/цифры/`_`/`-`, затем классифицируем как `Keyword`, `Operator` или `Identifier`.
     - Особый случай: если сразу после слова стоит `!` и `слово+"!"` — ключевое слово (например, `bello!`, `naidu!`) — поглощаем `!` и отдаём единый `Keyword`.
   - Число: возможен ведущий `-` для отрицательных, затем цифры, опционально дробная часть с точкой → `NumberLiteral`.
   - Строка `!…!`: берём срез исходного текста от открывающего `!` до закрывающего `!` включительно, сохраняя `!!` как два `!`. Внутри интерполяции особый случай `!)`: финальный фрагмент — `StringLiteral "!"`, скобка не поглощается строкой.
   - Строка `"…"`: берём срез вместе с кавычками → `StringLiteral`.
   - Разделители: одиночные `(` и `)` → `Delimiter`. Прочие подряд идущие небуквенно‑цифровые символы → один `Delimiter` токен.

### Классификация слов

- Ключевые слова по спецификации: `bello!`, `oca!`, `stopa`, `bapple`, `poop`, `trusela`, `bi-do`, `uh-oh`, `again`, `kemari`, `aspetta`, `tulalilloo`, `ti`, `amo`, `guoleila`, `tank`, `yu`, `boo-ya`, `naidu!`.
- Маркер `loka` используется в интерполяции: семантически это «прочие лексемы» при подсчёте статистики, но в токенах остаётся `Keyword` по спецификации примеров.
- Логические литералы: `Da`, `No`, а также строчные `da`, `no` — лексически ключевые слова.
- Имёна типов (`Banana`, `Papaya`, `Gelato`, `Spaghetti`) — лексически идентификаторы (их нельзя использовать как имена переменных, но лексер этого уровня это не запрещает). В статистике они учитываются как «прочие лексемы».
- Операторы — словесные: `lumai`, `beedo`, `dibotada`, `poopaye`, `pado`, `melomo`, `flavuk`, `con`, `la`, `looka`, `too`, `makoroni`, `tropa`, `bo-ca`.

### Позиции токенов

- Позиция — индекс первого символа лексемы в исходном тексте после нормализации перевода строк.
- Пробелы — разделители токенов. Например, после `!Hello, !` есть пробел, поэтому следующий `loka` начинается на позицию +1 от окончания строки.

### Комментарии

- `//` до конца строки — `Comment` (без необходимости `naidu!`).
- `/* … */` — многострочный `Comment`. Отсутствие закрытия не приводит к исключению — комментарий вытягивается до конца входа.

### Статистика (LexicalStats)

Метод `string CollectFromFile(string path)`:
- Читает файл как UTF‑8, передаёт текст лексеру, итерирует токены.
- Счёт по категориям в фиксированном порядке:
  - `keywords` — все ключевые слова, кроме стартового маркера `bello!`. Маркер `loka` учитывается в «прочих лексемах».
  - `identifier` — считаются уникальные идентификаторы (distinct по имени), чтобы не завышать счётчик на повторяющихся именах.
  - `number literals` — числа.
  - `string literals` — строки `!…!` и `"…"`.
  - `operators` — словесные операторы (см. список выше).
  - `other lexemes` — всё остальное (скобки, зарезервированные имена типов, `loka` и пр.).
- `Whitespace` и `Comment` в статистику не входят.

### Полный состав файлов и их элементы

- `src/Lexer/TokenType.cs`
  - `namespace Lexer`
  - `public enum TokenType { Identifier, Keyword, NumberLiteral, StringLiteral, Operator, Delimiter, Comment, Whitespace, EndOfFile, Unknown }`
  - Назначение каждого члена:
    - `Identifier` — имена переменных/функций/типов (лексически),
    - `Keyword` — ключевые слова управления и маркеры завершения/ввода‑вывода,
    - `NumberLiteral` — целые и дробные числа (включая отрицательные),
    - `StringLiteral` — строки `!…!` и `"…"`,
    - `Operator` — словесные операторы,
    - `Delimiter` — скобки и иные разделители,
    - `Comment` — `//` и `/* … */`,
    - `Whitespace` — последовательности пробельных символов,
    - `EndOfFile` — служебный тип (в текущем интерфейсе не эмитится наружу),
    - `Unknown` — резерв для неожиданной лексемы.

- `src/Lexer/Token.cs`
  - `namespace Lexer`
  - `public readonly record struct Token(TokenType Type, string Lexeme, int Position)` — см. выше.
  - `public override string ToString()` — формат: `"{Type}: '{Lexeme}' @ {Position}"`.

- `src/Lexer/Lexer.cs`
  - `namespace Lexer`
  - `public class Lexer`
    - Поля: `_source` (исходный текст, нормализованный по переводам строк), `_position` (текущий индекс), `_tokens` (накопитель токенов).
    - Конструктор: принимает строку (null → пустая строка), нормализует `\r\n`/`\r` к `\n`.
    - `public IReadOnlyList<Token> Tokenize()` — основной цикл лексинга, возвращает список токенов без `EndOfFile` (тесты фильтруют `Whitespace`).
    - Приватные методы чтения фрагментов:
      - `ReadWhitespace()` — группирует пробелы/табы/переводы строк, создаёт `Whitespace`.
      - `ReadWordLike()` — собирает слово (буквы/цифры/`_`/`-`),
        - если сразу за словом `!` и `word+"!"` — ключевое слово, поглощает `!` и создаёт `Keyword` (например, `bello!`, `naidu!`),
        - иначе проверяет на оператор → `Operator`,
        - иначе проверяет на ключевое слово → `Keyword`,
        - иначе `Identifier`.
      - `ReadNumber()` — поддержка `-` перед цифрой, целая/дробная части → `NumberLiteral`.
      - `ReadBangString()` — строки `!…!` по срезу исходника; `!!` сохраняется как `!!`; специальный случай финала интерполяции `!)` — даёт строку `"!"`, `)` не поглощается.
      - `ReadQuotedString()` — строки `"…"` по срезу исходника.
      - `ReadSingleLineComment()` — `//…\n` → `Comment`.
      - `ReadMultiLineComment()` — `/*…*/` или до конца входа → `Comment`.
      - `ReadDelimiterOrSymbol()` — `(`, `)` как отдельные `Delimiter`; прочие небуквенно‑цифровые подряд → один `Delimiter`.
    - Служебные методы:
      - `Peek(int offset = 0)` — просмотр символа на позиции `position + offset` (или `\0` за концом).
      - `Advance(int count = 1)` — сдвиг позиции вперёд с ограничением по длине.
      - `IsAtEnd()` — проверка конца входа.
      - `IsStartOfSingleLineComment()` / `IsStartOfMultiLineComment()` — быстрые проверки префиксов.
      - `IsKeyword(string)` / `IsOperator(string)` — проверка в таблицах языка (строгое сравнение `Ordinal`).

- `src/Lexer/LexicalStats.cs`
  - `namespace Lexer`
  - `public struct LexicalData` — счётчики категорий и форматированный `ToString()` в фиксированном порядке строк.
  - `public static class LexicalStats`
    - `public static string CollectFromFile(string path)`:
      - читает файл, лексирует,
      - считает категории: `bello!` исключён из `keywords`; `loka` и имена типов (`Banana`, `Papaya`, `Gelato`, `Spaghetti`) идут в `other lexemes`;
      - для `identifier` используется подсчёт уникальных имён (HashSet) — по ТЗ и примерам тестов.

- `src/Lexer/LexerException.cs`
  - `namespace SqlLexer` (исторический неймспейс — допустимо привести к `Lexer` при рефакторинге)
  - `public class LexerException : Exception` с `public int Position { get; }` и конструктором, формирующим сообщение `"{message} (pos={position})"`.

- `src/Lexer/TokenValue.cs`
  - `namespace Lexer`
  - `public class TokenValue` с тремя взаимно исключающимися полями‑значениями, конструкторами и `ToString()`.

### Конструкции C# в коде и их смысл

- `namespace` — логическая группировка типов; позволяет избегать конфликтов имён.
- `public/internal/private` — уровни доступа: во внешних сборках/внутри сборки/внутри типа.
- `class/struct/record/record struct` — описание типов (см. выше отличия по ссылочной/значимой семантике и value‑equality).
- `readonly` у `record struct` — поля неизменяемы после инициализации (иммутабельность).
- `override` — переопределение виртуального метода базового типа (`object.ToString`).
- `static` — тип/метод без необходимости создавать экземпляр (утилитарные функции, как `LexicalStats`).
- `ReadOnlySpan<string> = new[] { … }` — компактное объявление неизменяемого массива строк с быстрым перебором.
- `StringComparison.Ordinal` — бинарное сравнение по кодовым точкам (быстро и детерминировано, удобно для языка с фиксированным регистром ключевых слов).

### Производительность и границы

- Лексер однопроходный, без отката назад; допускает произвольный look‑ahead через `Peek(offset)`.
- Время — линейное от длины входа (O(n)); память — пропорциональна количеству токенов.
- Не выполняется интерпретация Escape‑последовательностей внутри `"…"` (кроме буквального включения `\n` внутри `!…!` — оно сохраняется как два символа `\\` и `n` в лексеме, чтобы совпасть с ожидаемым текстом тестов). При необходимости легко расширить.

### Почему такие решения

- `record struct` для `Token`:
  - value‑type без аллокаций в куче,
  - авто‑сравнение по значению и удобный `ToString` — идеально для тестов и логов,
  - `readonly` предотвращает мутацию и позволяет компилятору оптимизировать использование.

- Хранить лексему как исходный срез (`!…!`, `"…"`) проще и надёжнее: совпадают позиции и ожидаемый текст из тестов/спецификации.

- Нормализация перевода строк к `\n` исключает платформенные сдвиги индексов и делает тесты детерминированными.

### Частые вопросы

- «Почему типы — идентификаторы?» — Лексически это слова, а не операторы/ключевые слова (по грамматике), так что тип токена `Identifier`. Семантический запрет будет на этапе синтаксиса/семантики. В статистике, по ТЗ, типы исключены из `identifier` и идут в «прочие лексемы».

- «Зачем `override ToString`?» — Так ассерты и логи показывают тип/лексему/позицию в человекочитаемом виде, ускоряя отладку.

- «Почему `loka` не в `keywords` для статистики?» — По ТЗ это маркер интерполяции, не оператор выполнения, поэтому он учитывается как прочая лексема.

### Расширения/следующие шаги

- Добавить строгие ошибки для незакрытых строк/комментариев (с указанием позиции) — через `LexerException`.
- Вынести словари ключевых слов/операторов в конфигурацию/ресурсы, если потребуется локализация/расширение языка.
- Добавить категории для скобок/разделителей в статистике, если это потребуется аналитике.



